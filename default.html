<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML>
<HEAD>
	<META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset=utf-8">
	<TITLE>smallfile distributed I/O benchmark | Red Hat Intranet</TITLE>
	<META NAME="GENERATOR" CONTENT="LibreOffice 3.5  (Linux)">
	<META NAME="AUTHOR" CONTENT="anonymous">
	<META NAME="CREATED" CONTENT="0;0">
	<META NAME="CHANGEDBY" CONTENT="Ben England">
	<META NAME="CHANGED" CONTENT="20130322;17102500">
	<META NAME="category-departments" CONTENT="Engineering">
	<META NAME="category-keywords" CONTENT="filesystem">
	<META NAME="category-offices" CONTENT="Westford, MA">
	<META NAME="category-wiki-page-type" CONTENT="Misc">
	<META NAME="modified" CONTENT="2012-06-08 11:49:32">
	<META NAME="status" CONTENT="1">
	<META NAME="type" CONTENT="wiki_page">
	<STYLE TYPE="text/css">
	<!--
		@page { margin: 0.79in }
		P { color: #000000; font-family: "Liberation Serif", "Times New Roman", serif; font-size: 11pt; line-height: 138% }
		H1 { margin-top: 0.1in; margin-bottom: 0in; border: none; padding: 0in; color: #000000; font-family: "Liberation Sans", "Lucida Grande", "Helvetica", sans-serif; font-size: 22pt }
		H2 { margin-top: 0.1in; margin-bottom: 0in; border: none; padding: 0in; color: #000000; font-family: "Liberation Sans", "Lucida Grande", "Helvetica", sans-serif; font-size: 10pt; font-weight: normal; line-height: 130% }
		H1.with-tabs { font-family: "Liberation Sans", "Lucida Grande", "Helvetica", sans-serif; font-size: 22pt }
		PRE { color: #000000 }
		PRE.cjk { font-family: "WenQuanYi Zen Hei", monospace }
		PRE.ctl { font-family: "Lohit Devanagari", monospace }
		A:link { color: #003399; text-decoration: none }
		A:visited { color: #000000 }
	-->
	</STYLE>
</HEAD>
<BODY LANG="en-US" TEXT="#000000" LINK="#003399" VLINK="#000000" DIR="LTR">
<FORM ACTION="/wiki/smallfile-distributed-io-benchmark" METHOD="POST" ENCTYPE="multipart/form-data">
	<INPUT TYPE=HIDDEN NAME="form_build_id" VALUE="form-5c8f33b57ec17d3fc622343ca04cc920">
	<INPUT TYPE=HIDDEN NAME="form_token" VALUE="79eae801c0b52735704736049baaa583">
	<INPUT TYPE=HIDDEN NAME="form_id" VALUE="subscriptions_ui_node_form">
</FORM>
<FORM ACTION="/comment/reply/71422" METHOD="POST">
	<INPUT TYPE=HIDDEN NAME="form_build_id" VALUE="form-5d38f8d7377558dd0b9728158589d8f4">
	<INPUT TYPE=HIDDEN NAME="form_token" VALUE="d88d096c160d9b2ea61f976935dd016c">
	<INPUT TYPE=HIDDEN NAME="form_id" VALUE="comment_form">
</FORM>
<DIV ID="content-wrapper" DIR="LTR" STYLE="background: #dfe1e4">
	<P><BR><BR>
	</P>
	<DIV ID="inner-wrap" DIR="LTR">
		<P><BR><BR>
		</P>
		<DIV ID="center" DIR="LTR">
			<P><BR><BR>
			</P>
			<DIV ID="tabs-wrapper" DIR="LTR">
				<H1 CLASS="with-tabs" STYLE="margin-top: 0in; margin-bottom: 0.2in; border: none; padding: 0in">
				smallfile distributed I/O benchmark</H1>
			</DIV>
			<DIV ID="node-71422" DIR="LTR">
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">&nbsp;This
				page describes the <STRONG>smallfile</STRONG> benchmark program.&nbsp;
				It is a python-based small-file distributed POSIX workload
				generator which can be used to quickly measure performance for a
				variety of metadata-intensive workloads across an entire
				cluster.&nbsp;&nbsp; It has no dependencies on any specific
				filesystem or implementation AFAIK.&nbsp; It is intended to
				complement use of iozone benchmark for measuring performance of
				large-file workloads, and borrows certain concepts from iozone
				and Ric Wheeler's fs_mark.&nbsp;&nbsp;&nbsp; It was developed by
				Ben England starting in March 2009, and is now open-source.
				Here's an example of the kind of data that can be generated with
				it:</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><IMG SRC="default_files/glusterfs-smallfile-2.jpg" NAME="graphics2" ALIGN=BOTTOM WIDTH=725 HEIGHT=614 BORDER=0></P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">Capabilities
				include:</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">- can
				manage workload generator processes on multiple hosts</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">-
				calculates aggregate throughput for entire set of hosts</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">- can
				start and stop all workload generator processes at approximately
				the same time (necessary for accurate aggregate throughput
				measurement)</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">-
				useful for generating &quot;pure&quot; workloads (for example,
				just creates, or deletes, or setattr)</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">- easy
				to extend to new workload types</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">-
				provides CLI for scripted use, but workload generator is separate
				from CLI and can also be invoked directly from a GUI if desired</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">-
				supports either fixed file size or random exponential file size</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">- can
				capture response time data in .csv format, provides utility to
				reduce this data to statistics</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">- has
				run successfully on Windows, though this functionality is not
				regularly tested at present.</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">-
				writes unique data pattern in all files, verifies data read
				against this pattern</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">- in
				multi-host tests, can force all clients to read files written by
				different client</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">For a
				multi-host test, ALL hosts <EM>must provide access to the same
				shared directory</EM> with pathname.</P>
				<H1 STYLE="margin-top: 0in; margin-bottom: 0.2in">Restrictions</H1>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">untested
					on python 3, probably doesn't run yet (exception syntax) 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">does
					not work on laptop when hostname is not resolvable via DNS (e.g.
					dhcp-185-182) 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">is
					not accurate on memory-resident filesystem on single host 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">requires
					all hosts to have same DNS domain name (plan to remove this
					restriction) 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">does
					not support HTTP access 
					</P>
				</UL>
				<H1 STYLE="margin-top: 0in; margin-bottom: 0.2in">How to run</H1>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">You
				must have password-less ssh access between the test driver node
				and the workload generator hosts if you want to run a distributed
				(multi-host) test.</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">You
				must use a directory visible to all participating hosts to run a
				distributed test.</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">To see
				what parameters are supported by smallfile_cli.py, do &quot;python
				smallfile_cli.py -h&quot;. Boolean true/false parameters can be
				set to either Y (true) or N (false). Every command consists of a
				sequence of parameter name-value pairs with the format –<B>name
				value</B> .</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">The
				parameters are:</P>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--operation</B>
					-- operation name, one of the following: 
					</P>
					<UL>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">create
						-- create a file and write data to it 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">append
						-- open an existing file and append data to it 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">delete
						-- delete a file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">rename
						-- rename a file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">delete_renamed
						-- delete a file that had previously been renamed 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">read
						-- read an existing file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">stat
						-- just read metadata from an existing file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">chmod
						-- change protection mask for file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">setxattr
						-- set extended attribute values in each file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">getxattr
						- read extended attribute values in each file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">symlink
						-- create a symlink pointing to each file (create must be run
						beforehand) 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">mkdir
						-- create a subdirectory with 1 file in it 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">rmdir
						-- remove a subdirectory and its 1 file 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">readdir
						– scan directories only, don't read files or their metadata</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">ls-l
						– scan directories and read basic file metadata</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">cleanup
						-- delete any pre-existing files from a previous run 
						</P>
						<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">all
						-- create, append, read, rename and delete each file. 
						</P>
					</UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--files</B>
					-- how many files should each thread process?&nbsp; 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--threads</B>
					-- how many workload generator threads should each
					invocation_cli process create? 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--file-size</B>
					-- total amount of data accessed per file. &nbsp; If zero then
					no reads or writes are performed. 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">-<B>-file-size-distribution</B>
					– only supported value today is <B>exponential</B>, default
					value is fixed file size.</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--record-size</B>
					-- record size in KB, how much data is transferred in a single
					read or write system call.&nbsp; If 0 then it is set to the file
					size. 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--top</B>
					-- top-level directory, all file accesses are done inside this
					directory tree. If you wish to use multiple mountpoints,provide
					a list of top-level directories separated by comma (no
					whitespace).</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--host-set</B>
					-- comma-separated set of hosts used for this test (default is
					current host name), no domain names allowed 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--files-per-dir</B>
					-- maximum number of files contained in any one directory 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--dirs-per-dir</B>
					-- maximum number of subdirectories contained in any one
					directory 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--hash-into-dirs</B>
					– default N, if Y then assign next file to a directory using a
					hash function, otherwise assign next –files-per-dir files to
					next directory</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--permute-host-dirs</B>
					– default N, if Y then have each host process a different
					subdirectory tree than it otherwise would (see below for
					directory tree structure).</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">-<B>-same-dir</B>
					-- boolean, default N, if Y then threads will share a single
					directory</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--network-sync-dir</B>
					– don't need to specify unless you run a multi-host test and
					the –top parameter points to a non-shared directory (see
					discussion below)</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--xattr-size</B>
					-- size of extended attribute value in bytes (names begin with
					'user.smallfile-') 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--xattr-count</B>
					-- number of extended attributes per file 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--prefix</B>
					-- a string prefix to prepend to files (so they don't collide
					with previous runs for example) 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--suffix</B>
					-- a string suffix to append to files (so they don't collide
					with previous runs for example) 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--finish</B>
					-- boolean, default Y, if Y, thread will complete all requested
					file operations even if measurement has finished 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--stonewall</B>
					-- boolean, default Y, if Y then thread will measure throughput
					as soon as it detects that another thread has finished 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--remote-pgm-dir
					</B>– don't need to specify this unless the smallfile software
					lives in a different directory on the target hosts and the
					test-driver host. 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>--pause
					</B>-- integer (microseconds), default 0, if non-zero each
					thread will delay for this many microseconds before starting
					next file 
					</P>
				</UL>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">So for
				example, if you want to run <STRONG>smallfile_cli.py</STRONG> on
				1 host with 8 threads each creating 2 GB of 1-MB files, you can
				use these options:</P>
				<PRE CLASS="western" STYLE="margin-bottom: 0.2in; border: none; padding: 0in"><STRONG> </STRONG><STRONG><FONT SIZE=3># python smallfile_cli.py --operation create --threads 8 --file-size 1024 --files 2097 --top /mnt/gfs/smf</FONT></STRONG></PRE><P STYLE="margin-bottom: 0in; border: none; padding: 0in">
				To run a 4-host test doing same thing:</P>
				<PRE CLASS="western" STYLE="border: none; padding: 0in"><STRONG> </STRONG><STRONG><FONT SIZE=3># python smallfile_cli.py --operation create --threads 8 --file-size 1024 --files 2097 --top /mnt/gfs/smf \</FONT></STRONG>
<STRONG>     </STRONG><STRONG><FONT SIZE=3>--host-set host1,host2,host3,host4</FONT></STRONG> </PRE><P STYLE="margin-bottom: 0in; border: none; padding: 0in">
				Errors encountered by worker threads will be saved in
				<B>/var/tmp/invoke-N.log</B> where <B>N</B> is the thread number.
				After each test, a summary of thread results is displayed, and
				overall test results are aggregated for you, in three ways:</P>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><I>files/sec</I>
					– only metric relevant to all tests</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><I>IOPS</I>
					– application I/O operations per second, rate at which
					benchmark performed reads/writes</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><I>MB/s</I>
					– megabytes/sec, rate at which application transferred data</P>
				</UL>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">Users
				should never need to run <STRONG>smallfile.py</STRONG> -- this is
				the python class which implements the workload generator.
				Developers can run this module to invoke its unit test however:</P>
				<PRE CLASS="western" STYLE="margin-bottom: 0.2in; border: none; padding: 0in"><STRONG> </STRONG><STRONG><FONT SIZE=3># python smallfile.py </FONT></STRONG></PRE><P STYLE="margin-bottom: 0in; border: none; padding: 0in">
				To run just one unit test module run</P>
				<PRE CLASS="western" STYLE="margin-bottom: 0.2in; border: none; padding: 0in"><STRONG> </STRONG><STRONG><FONT SIZE=3># python -m unittest smallfile.Test.test_c3_Symlink</FONT></STRONG></PRE><P STYLE="margin-bottom: 0in; line-height: 100%">
				<BR>
				</P>
				<P STYLE="margin-bottom: 0in; line-height: 100%"><FONT FACE="Liberation Serif, serif"><FONT SIZE=4 STYLE="font-size: 16pt">Use
				with non-networked filesystems</FONT></FONT></P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">There
				are cases where you want to use a distributed filesystem test on
				host-local filesystems. One such example is virtualization, where
				the “local” filesystem is really layered on a virtual disk
				image which may be stored in a network filesystem. The benchmark
				needs to share certain files across hosts to return results and
				synchronize threads. In such a case, you specify the
				–<B>network-sync-dir</B> <I>directory-pathname</I><SPAN STYLE="font-variant: normal">
				</SPAN><SPAN STYLE="font-variant: normal"><SPAN STYLE="font-style: normal">parameter
				to have the benchmark use a directory in some shared filesystem
				external to the test directory (specified with </SPAN></SPAN><SPAN STYLE="font-variant: normal">–</SPAN><SPAN STYLE="font-variant: normal"><SPAN STYLE="font-style: normal"><B>top</B></SPAN></SPAN><SPAN STYLE="font-variant: normal">
				</SPAN><SPAN STYLE="font-variant: normal"><SPAN STYLE="font-style: normal">parameter).
				By default, if this parameter is not specified then the shared
				directory will be the subdirectory </SPAN></SPAN><SPAN STYLE="font-variant: normal"><SPAN STYLE="font-style: normal"><B>network-dir</B></SPAN></SPAN><SPAN STYLE="font-variant: normal">
				</SPAN><SPAN STYLE="font-variant: normal"><SPAN STYLE="font-style: normal">underneath
				the directory specified with the </SPAN></SPAN><SPAN STYLE="font-variant: normal">–</SPAN><SPAN STYLE="font-variant: normal"><SPAN STYLE="font-style: normal"><B>top</B></SPAN></SPAN><SPAN STYLE="font-variant: normal">
				</SPAN><SPAN STYLE="font-variant: normal"><SPAN STYLE="font-style: normal">parameter.</SPAN></SPAN></P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><BR><BR>
				</H2>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in; border: none; padding: 0in">
				<FONT SIZE=4 STYLE="font-size: 16pt">Use of --pause in
				multi-thread tests</FONT></H2>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">In some
				filesystems, the first thread that starts running will be
				operating at memory speed (example: NFS writes) and can easily
				finish before other threads have a chance to get started.&nbsp;
				This immediately invalidates the test.&nbsp; To make this less
				likely, it is possible to insert a per-file delay into each
				thread with the --pause option so that the other threads have a
				chance to participate in the test during the measurement
				interval.&nbsp;&nbsp;&nbsp; It is preferable to run a longer test
				instead, because in some cases you might otherwise restrict
				throughput unintentionally.&nbsp; But if you know that your
				throughput upper bound is X files/sec and you have N threads
				running, then your per-thread throughput should be no more than
				N/X, so a reasonable pause would be something like 3X/N
				microseconds.&nbsp; For&nbsp; example, if you know that you
				cannot do better than 100000 files/sec and you have 20 threads
				running,try a 60/100000 = 600 microsecond pause.&nbsp; Verify
				that this isn't affecting throughput by reducing the pause and
				running a longer test.</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><FONT SIZE=4 STYLE="font-size: 16pt">Use
				of subdirectories</FONT></H2>
				<P>Before a test even starts, the smallfile benchmark ensures
				that the directories needed by that test already exist (there is
				a specific operation type for testing performance of subdirectory
				creation and deletion). If the top directory (specified by –top
				parameter) is T, then the top per-thread directory is T/host/dTT
				where TT is a 2-digit thread number and “host” is the
				hostname. If the test is not a distributed test, then it's just
				whatever host the benchmark command was issued on, otherwise it
				is each of the hosts specified by the –host-set parameter. The
				first F files (where F is the value of the –files-per-dir)
				parameter are placed in this top per-thread directory. If the
				test uses more than F files/thread, then at least one
				subdirectory from the first level of subdirectories must be used;
				these subdirectories have the path T/host/dTT/dNNN where NNN is
				the subdirectory number. Suppose the value of the parameter
				–subdirs-per-dir is D. Then there are at most D subdirectories
				of the top per-thread directory. If the test requires more than
				D(F+1) files per thread, then a second level of subdirectories
				will have to be created, with pathnames like T/host/dTT/dNNN/dMMM
				. This process of adding subdirectories continues in this fashion
				until there are sufficient subdirectories to hold all the files.
				The purpose of this approach is to simulate a mixture of
				directories and files, and to not require the user to specify how
				many levels of directories are required.</P>
				<P>The use of multiple mountpoints is supported. This features is
				useful for testing NFS, etc.</P>
				<P>Note that the test harness does not have to scan the
				directories to figure out which files to read or write – it
				simply generates the filename sequence itself.  If you want to
				test directory scanning speed, use <B>readdir</B> or <B>ls-l</B>
				operations. 
				</P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><FONT SIZE=4 STYLE="font-size: 16pt">Sharing
				directories across threads</FONT></H2>
				<P><BR><BR>
				</P>
				<P>Some applications require that many threads, possibly spread
				across many host machines, need to share a set of directories.
				The <B>--same-dir</B> parameter makes it possible for the
				benchmark to test this situation. By default this parameter is
				set to N, which means each thread has its own non-overlapping
				directory tree. This setting provides the best performance and
				scalability. However, if the user sets this parameter to Y, then
				the top per-thread directory for all threads will be <B>T </B>instead
				of <B>T/host/dTT</B> as described in preceding section.</P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><FONT SIZE=4 STYLE="font-size: 16pt">Hashing
				files into directory tree</FONT></H2>
				<P><FONT SIZE=2 STYLE="font-size: 11pt">For applications which
				create very large numbers of small files (millions for example),
				it is impossible or at the very least impractical to place them
				all in the same directory, whether or not the filesystem supports
				so many files in a single directory. There are two ways which
				applications can use to solve this problem:</FONT></P>
				<UL>
					<LI><P><FONT SIZE=2 STYLE="font-size: 11pt">insert files into 1
					directory at a time – can create I/O and lock contention for
					the directory metadata</FONT></P>
					<LI><P><FONT SIZE=2 STYLE="font-size: 11pt">insert files into
					many directories at the same time – relieves I/O and lock
					contention for directory metadata, but increases the amount of
					metadata caching needed to avoid cache misses</FONT></P>
				</UL>
				<P><FONT SIZE=2 STYLE="font-size: 11pt">The –<B>hash-into-dirs</B>
				parameter is intended to enable simulation of this latter mode of
				operation. By default, the value of this parameter is N, and in
				this case a smallfile thread will sequentially access directories
				one at a time. In other words, the first D (where D = value of
				–<B>files-per-dir </B>parameter) files will be assigned to the
				top per-thread directory, then the next D files will be assigned
				to the next per-thread directory, and so on. However, if the
				–<B>hash-into-dirs</B> parameter is set to Y, then the number
				of the file being accessed by the thread will be hashed into the
				set of directories that are being used by this thread. </FONT>
				</P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><FONT SIZE=4 STYLE="font-size: 16pt">Random
				file size distribution option</FONT></H2>
				<P STYLE="margin-bottom: 0.2in"><FONT SIZE=2 STYLE="font-size: 11pt">In
				real life, users don't create files that all have the same size.
				Typically there is a file size distribution with a majority of
				small files and a lesser number of larger files. This benchmark
				supports use of the random exponential distribution to
				approximate that behavior. If you specify </FONT>
				</P>
				<P STYLE="margin-left: 0.79in; margin-bottom: 0.2in">–<FONT FACE="Courier 10 Pitch"><FONT SIZE=2 STYLE="font-size: 11pt">file-size-distribution
				</FONT></FONT><FONT FACE="Courier 10 Pitch"><FONT SIZE=2 STYLE="font-size: 11pt"><B>exponential</B></FONT></FONT><FONT FACE="Courier 10 Pitch"><FONT SIZE=2 STYLE="font-size: 11pt">
				–file-size </FONT></FONT><FONT FACE="Courier 10 Pitch"><FONT SIZE=2 STYLE="font-size: 11pt"><B>S</B></FONT></FONT><FONT FACE="Courier 10 Pitch"><FONT SIZE=2 STYLE="font-size: 11pt">
				</FONT></FONT>
				</P>
				<P STYLE="margin-bottom: 0.2in"><FONT SIZE=2 STYLE="font-size: 11pt">The
				meaning of the –<B>file-size</B> parameter changes to the
				<I>maximum</I> file size (<B>S</B> KB), and the mean file size
				becomes <B>S</B>/8. All file sizes are rounded down to the
				nearest kilobyte boundary, and the smallest allowed file size is
				1 KB. When this option is used, the smallfile benchmark saves the
				seed for each thread's random number generator object in a <B>.seed</B>
				file stored in the <B>TMPDIR</B> directory (typically <B>/var/tmp</B>).
				This allows the file reader to recreate the sequence of random
				numbers used by the file writer to generate file sizes, so that
				the reader knows exactly how big each file should be without
				asking the file system for this information. The append operation
				works in the same way. All other operations are metadata
				operations and do not require that the file size be known in
				advance.</FONT></P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><FONT SIZE=4 STYLE="font-size: 16pt">Avoiding
				caching effects</FONT></H2>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><A NAME="Avoiding_caching_effects"></A>
				THere are two types of caching effects that we wish to avoid,
				data caching and metadata caching.&nbsp; If the average object
				size is sufficiently large, we need only be concerned about data
				caching effects.&nbsp; In order to avoid data caching effects
				during a large-object read test, the Linux buffer cache on all
				servers must be cleared. In part this is done using the command:
				&quot;echo 1 &gt; /proc/sys/vm/drop_caches&quot; on all hosts.&nbsp;
				However, gluster has its own internal caches.&nbsp;&nbsp; To
				evict all prior data from the cache, the simplest method is to
				just use iozone to write a large amount of data into some files
				in the gluster filesystem, then delete them.&nbsp; For example,
				if the gluster 3.2 server caches 1 GB of data then the amount of
				data written should be roughly 2 GB/server and the number of
				files used should be roughly 8 times the number of servers.&nbsp;
				Use of many separate files ensures that this cache eviction data
				is spread across all servers approximately equally.<STRONG>&nbsp;</STRONG></P>
				<H1 STYLE="margin-top: 0in; margin-bottom: 0.2in">Results</H1>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">All
				tests display a &quot;files/sec&quot; result.&nbsp; If the test
				performs reads or writes, then a &quot;MB/sec&quot; data transfer
				rate and an &quot;IOPS&quot; result (i.e. total read or write
				calls/sec) are also displayed.&nbsp; Each thread participating in
				the test keeps track of total number of files and I/O requests
				that it processes during the test measurement interval.&nbsp;
				These results are rolled up per host if it is a single-host
				test.&nbsp; For a multi-host test, the per-thread results for
				each host are saved in a file within the --top directory, and the
				test master then reads in all of the saved results from its
				slaves to compute the aggregate result across all client hosts.&nbsp;
				The percentage of requested files which were processed in the
				measurement interval is also displayed, and if the number is
				lower than a threshold (default 70%) then an error is raised.</P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in; border: none; padding: 0in">
				Response time collection</H2>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">response
				times for operations on each file are saved by thread in .csv
				form. &nbsp; For example, you can turn these into an X-Y
				scatterplot so that you can see how response time varies over
				time, to use:<BR>&nbsp;&nbsp;</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><STRONG><FONT SIZE=4>#
				python smallfile_cli.py --response-times Y</FONT></STRONG><BR><STRONG><FONT SIZE=4>#
				ls -ltr /var/tmp/rsptimes*.csv</FONT></STRONG></P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">You
				should see 1 .csv file per thread.&nbsp; These files should be in
				a format<BR>that can be loaded into any spreadsheet application,
				such as Excel, and<BR>graphed.&nbsp; An x-y scatterplot can be
				useful to see changes over time in response time.</P>
				<H1 STYLE="margin-top: 0in; margin-bottom: 0.2in">Background</H1>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">There
				are many existing performance test benchmarks. I have tried just
				about all the ones that I've heard of. Here are the ones I have
				looked at:</P>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>Bonnie++</B>
					-- works well for a single host, but you cannot generate load
					from multiple hosts because the benchmark will not synchronize
					its activities, so different phases of the benchmark will be
					running at the same time, whether you want them to or not. 
					</P>
				</UL>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>iozone</B>
					-- this is a great tool for large-file testing, but it can only
					do 1 file/thread in its current form. 
					</P>
				</UL>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>postmark</B>
					-- works fine for a single client, not as useful for
					multi-client tests 
					</P>
				</UL>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>grinder</B>
					-- has not to date been useful for filesystem testing, though it
					works well for web services testing. 
					</P>
				</UL>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>JMeter</B>
					– has been used successfully by others in the past. 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>fs_mark</B>
					-- Ric Wheeler's filesystem benchmark, is very good at creating
					files 
					</P>
				</UL>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in"><B>fio</B>
					-- Linux test tool -- broader coverage of Linux system calls
					particularly around async. and direct I/O 
					</P>
				</UL>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">&nbsp;</P>
				<H1 STYLE="margin-top: 0in; margin-bottom: 0.2in">Design
				principles</H1>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">A
				cluster-aware test tool has to:</P>
				<UL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">start
					threads on all hosts at same time 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">stop
					measurement of throughput for all threads at the same time 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">be
					easy to use in all file system environments 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">be
					highly portable 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">have
					very low overhead 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">not
					require threads to synchronize (be embarrassingly parallel) 
					</P>
				</UL>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">Although
				there may be some useful tests that involve thread
				synchronization or contention, but we don't want the tool to
				<I>force</I> thread synchronization or contention for resources.
				In order to run prolonged small-file tests (which is a
				requirement for scalability to very large clusters),, each thread
				has to be able to use more than one directory.&nbsp;&nbsp; Since
				some filesystems perform very differently as the files/directory
				ratio increases, and most applications and users do not rely on
				having huge file/directory ratios, this is also important for
				testing the filesystem with a realistic use case.&nbsp; This
				benchmark does something similar to Ric Wheeler's fs_mark
				benchmark with multiple directory levels.&nbsp;&nbsp; This
				benchmark imposes no hard limit on how many directories can be
				used and how deep the directory tree can go.&nbsp; Instead, it
				creates directories according to these constraints:</P>
				<OL>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">files
					(and directories) are placed as close to the root of the
					directory hierarchy as possible 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">no
					directory contains more than the number of files specified in
					the --files-per-dir test parameter 
					</P>
					<LI><P STYLE="margin-bottom: 0in; border: none; padding: 0in">no
					directory contains more than number of subdirectories specified
					in the --dirs-per-dir test parameter 
					</P>
				</OL>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><BR><BR>
				</H2>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in"><A NAME="Synchronization"></A>
				<FONT SIZE=4 STYLE="font-size: 16pt">Synchronization</FONT></H2>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">A
				single directory is used to synchronize the threads and hosts.
				This may seem problematic, but we assume here that the file
				system is not very busy when the test is run (otherwise why would
				you run a load test on it?). So if a file is created by one
				thread, it will quickly be visible on the others, as long as the
				filesystem is not heavily loaded. 
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">If it's
				a single-host test, any directory is sharable amongst threads,
				but in a multi-host test only a directory shared by all
				participating hosts can be used. If the –<B>top</B> test
				directory is in a network-accessible file system (could be NFS or
				Gluster for example), then the synchronization directory is by
				default in the network_shared subdirectory by default and need
				not be specified. If the –<B>top</B> directory is in a
				host-local filesystem, then the –<B>network-sync-dir</B> option
				must be used to specify the synchronization directory. When a
				network directory is used, change propagation between hosts
				cannot be assumed to occur in under two seconds. 
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">We use
				the concept of a &quot;starting gate&quot; -- each thread does
				all preparation for test, then waits for a special file, the
				&quot;starting gate&quot;, to appear in the shared area. When a
				thread arrives at the starting gate, it announces its arrival by
				creating a filename with the host and thread ID embedded in it.
				When all threads have arrived, the controlling process will see
				all the expected &quot;thread ready&quot; files, and will then
				create the <B>starting gate</B> file. When the starting gate is
				seen, the thread pauses for a couple of seconds, then commences
				generating workload. This initial pause reduces time required for
				all threads to see the starting gate, thereby minimizing chance
				of some threads being unable to start on time. Synchronous thread
				startup reduces the &quot;warmup time&quot; of the system
				significantly.</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">We also
				need a checkered flag (borrowing from car racing metaphor). Once
				test starts, each thread looks for a <B>stonewall</B> file in the
				synchronization directory. If this file exists, then the thread
				stops measuring throughput at this time (but can (and does by
				default) optionally continue to perform requested number of
				operations). Consequently throughput measurements for each thread
				may be added to obtain an accurate aggregate throughput number.
				This practice is sometimes called &quot;stonewalling&quot; in the
				performance testing world.</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in">Synchronization
				operations do not require the worker threads to read the
				synchronization directory. For distributed tests, the test driver
				host has to check whether the various per-host synchronization
				files exist, but this does not require a readdir operation. The
				test driver does this check in such a way that the number of file
				lookups is only slightly more than the number of hosts, and this
				does not require reading the entire directory, only doing a set
				of lookup operations on individual files, so it's <I>O(n)</I>
				scalable as well.</P>
				<P STYLE="margin-bottom: 0in; border: none; padding: 0in"><BR>
				</P>
				<H2 STYLE="margin-top: 0in; margin-bottom: 0.2in; border: none; padding: 0in">
				<FONT SIZE=4 STYLE="font-size: 16pt">How results are returned to
				master process</FONT></H2>
				<P>For either single-host or multi-host tests, each test thread
				is implemented as a <B>smf_invocation</B> object and all thread
				state is kept there.&nbsp; Results are returned by using python
				&quot;pickle&quot; files to serialize the state of these
				per-thread objects containing details of each thread's progress
				during the test.&nbsp; The pickle files are stored in the shared
				synchronization directory. This restriction could be lifted if it
				was important enough but that's how it works today.<SCRIPT SRC="default_files/wysiwyg_002.js"></SCRIPT><SCRIPT>
<!--//--><![CDATA[//><!--
var _paq = _paq || [];(function(){var u=(("https:" == document.location.protocol) ? "https://metrics.corp.redhat.com/" : "https://metrics.corp.redhat.com/");_paq.push(["setSiteId", 1]);_paq.push(["setTrackerUrl", u+"piwik.php"]);_paq.push(["setDocumentTitle", "Home/smallfile+distributed+I%2FO+benchmark"]);_paq.push(["trackPageView"]);_paq.push(["enableLinkTracking"]);var d=document,g=d.createElement("script"),s=d.getElementsByTagName("script")[0];g.type="text/javascript";g.defer=true;g.async=true;g.src=u+"piwik.js";s.parentNode.insertBefore(g,s);})();
//--><!]]>
</SCRIPT></P>
			</DIV>
		</DIV>
	</DIV>
</DIV>
</BODY>
</HTML>